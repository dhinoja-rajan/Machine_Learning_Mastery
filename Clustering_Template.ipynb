{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrhWXZ4TnRdiACrmZ9FcVN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajan-dhinoja/Machine_Learning_Mastery/blob/main/Clustering_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Clustering Template**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KBPGWkUlq5PL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvALcJ3yAryu"
      },
      "source": [
        "# ***Step-A: Data Preprocessing:-***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbMfPnCfAryw"
      },
      "source": [
        "## Step-1: Import Required Dependencies:-\n",
        "Import essential libraries and modules for data manipulation, visualization, and preprocessing..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set the option to prevent silent downcasting\n",
        "np.set_printoptions(formatter={'float': '{:,.2f}'.format}, suppress=True, precision=2)\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "pd.set_option('future.no_silent_downcasting', True)\n",
        "\n",
        "print(\"✅ Required Libraries are Imported - Done!\\n\")"
      ],
      "metadata": {
        "id": "MmgVQ5oeAryx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVRL8v08Aryx"
      },
      "source": [
        "## Step-2: Load the Dataset and Display Different overviews of Datasets:-"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# dataset = pd.DataFrame(pd.read_csv('/content/drive/MyDrive/Datasets/CLUSTERING/mall_customers.csv'))\n",
        "# dataset.drop(columns=['CustomerID'], inplace=True)\n",
        "# dataset = pd.DataFrame(pd.read_csv('/content/drive/MyDrive/Datasets/CLUSTERING/online_retail/online_retail_1.csv'))\n",
        "# dataset = pd.DataFrame(pd.read_csv('/content/drive/MyDrive/Datasets/CLUSTERING/online_retail/online_retail_2.csv'))\n",
        "dataset = pd.DataFrame(pd.read_csv('/content/drive/MyDrive/Datasets/CLUSTERING/crime_data.csv'))\n",
        "# dataset = pd.DataFrame(pd.read_csv('/content/drive/MyDrive/Datasets/CLUSTERING/household_power_consumption.csv'))\n",
        "# X = dataset.iloc[:, [3, 4]].values\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "xBtYmKrSAryy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate the whole datset into categorical and numerical columns...\n",
        "\n",
        "categorical_cols = dataset.select_dtypes(include=['object']).columns\n",
        "numerical_cols = dataset.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "if categorical_cols.empty:\n",
        "  print(\"❌ No Categorical Columns Found...\")\n",
        "else:\n",
        "  print(\"✅ Categorical Columns found: \\n\", categorical_cols, \"\\n\")\n",
        "\n",
        "if numerical_cols.empty:\n",
        "  print(\"❌ No Numerical Columns Found...\")\n",
        "else:\n",
        "  print(\"✅ Numerical Columns found: \\n\", numerical_cols)"
      ],
      "metadata": {
        "id": "zcZDaNI0I-4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"> Shape of the Dataset: \", dataset.shape, \"\\n\")\n",
        "print(\"> Information about Dataset:\")\n",
        "print(dataset.info(), \"\\n\")\n",
        "print(\"> Statistical summary of the Dataset:\")\n",
        "# print(dataset.describe().map(lambda x: round(x, 4)))\n",
        "print(dataset.describe().to_string(header=True))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZW3HjieQAry2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(10, 6))\n",
        "# Plot target distribution\n",
        "sns.histplot(dataset.iloc[:, -1], bins=30, kde=True)\n",
        "plt.title('Distribution of Last Column')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_z7mUwRhXBoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Matrix Heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(dataset[numerical_cols].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix \\n',fontsize=20,  fontweight=800)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5bMrljUcI9zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-3:- Checking the Dataset:-"
      ],
      "metadata": {
        "id": "e0E0usXBAry4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step-3.1: Checking any Duplicate Data and Handling them:-"
      ],
      "metadata": {
        "id": "PSX5IX9RAry5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset.duplicated().any():\n",
        "  dataset.drop_duplicates(inplace=True)\n",
        "  print(\"✅ Duplicate Data(or Identical Rows) found and Removed...\")\n",
        "else:\n",
        "    print(\"❌ No Duplicate Data(or Identical Rows) found...\")"
      ],
      "metadata": {
        "id": "XzvdrZlWAry5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step-3.2: Cheking any Missing Data and Handling them:-"
      ],
      "metadata": {
        "id": "FKBEjTdDAry6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here from the module named impute of the library scikit-learn, we are using the SimpleImputer Class to Handle the Missing Values.\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "missing_data_counts = dataset.isnull().sum() + dataset.isin(['', 'N/A', 'Unknown', 'NaN']).sum()\n",
        "\n",
        "# if dataset.isnull().values.any() or dataset.isin(['', 'NaN', 'N/A', 'Unknown']).any().any():\n",
        "if missing_data_counts.any():\n",
        "\n",
        "  categorical_missing_counts = dataset[categorical_cols].isnull().sum() + dataset[categorical_cols].isin(['', 'N/A', 'Unknown', 'NaN']).sum()\n",
        "  numerical_missing_counts = dataset[numerical_cols].isnull().sum()\n",
        "\n",
        "  # Replace \"Unknown\" with NaN in categorical columns\n",
        "  for col in categorical_cols:\n",
        "    dataset[col] = dataset[col].replace('Unknown', np.nan)\n",
        "\n",
        "\n",
        "  # Check if there are any missing values (categorical or numerical)\n",
        "  if categorical_missing_counts.any() or numerical_missing_counts.any():\n",
        "      # Print missing counts for categorical columns in the desired format\n",
        "      print(\"⚠️ Missing Data Found! Handling them...\\n\")\n",
        "      print(\"Missing Data Counts in Categorical Columns: \\n\", categorical_missing_counts)\n",
        "      print(\"\\n\")\n",
        "      print(\"Missing Data Counts in Numerical Columns: \\n\", numerical_missing_counts)\n",
        "      print(\"\\n\")\n",
        "\n",
        "      # Create imputers for categorical and numerical features\n",
        "      categorical_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
        "      numerical_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "      # Apply imputers to the selected columns in X\n",
        "      if len(categorical_cols) > 0:\n",
        "        categorical_cols_for_impution = [col for col in categorical_cols if col != dataset.columns[-1]]\n",
        "        dataset[categorical_cols_for_impution] = categorical_imputer.fit_transform(dataset[categorical_cols_for_impution])\n",
        "\n",
        "      # Exclude the dependent variable column (last column) if it's numerical\n",
        "      numerical_cols_for_impution = [col for col in numerical_cols if col != dataset.columns[-1]]\n",
        "      if len(numerical_cols_for_impution) > 0:\n",
        "          dataset[numerical_cols_for_impution] = numerical_imputer.fit_transform(dataset[numerical_cols_for_impution])\n",
        "      # if len(numerical_cols) > 0:\n",
        "          # dataset[numerical_cols] = numerical_imputer.fit_transform(dataset[numerical_cols])\n",
        "      print(\"✅ Missing Data Handled Successfully...\")\n",
        "      # print(\"New Data with replaced missing values: \\n\", dataset.head(10).to_string(header=True))\n",
        "else:\n",
        "    print(\"❌ No missing data found...\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vg_Slpg2Ary7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step-3.3: Checking any Synonyms or Aliases and Handling them:-"
      ],
      "metadata": {
        "id": "-F0V-G0-Ary8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for col in dataset.columns:\n",
        "#   value_counts_dataset = dataset[col].value_counts().rename_axis('Unique Values: ').reset_index(name='Counts: ')\n",
        "\n",
        "#   print(f\"Column: \\t\\t'{col}'\")\n",
        "#   print(value_counts_dataset.T.to_string(header=False), \"\\n\")"
      ],
      "metadata": {
        "id": "rJwxtUSyAry8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step-3.4: Checking for Stopwords and Stemming them:-"
      ],
      "metadata": {
        "id": "szLznVnf1W1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# if categorical_cols.any():\n",
        "#   nltk.download('stopwords')\n",
        "#   stop_words = set(stopwords.words('english'))\n",
        "#   # stop_words = stop_words.remove('not')\n",
        "#   # print(\"\\n Stop Words are: \", stop_words, \"\\n\")\n",
        "#   def stemming(text):\n",
        "#       words = text.lower().split()\n",
        "\n",
        "#       stemmer = PorterStemmer()\n",
        "#       stemmed_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "#       return ' '.join(stemmed_words)  # Join stemmed words back into a string\n",
        "\n",
        "#   # Apply the stemming function to the specified columns\n",
        "#   for column in categorical_cols:\n",
        "#       dataset[column] = dataset[column].astype(str).apply(stemming)\n",
        "\n",
        "#   print(\"✅ Stemming Completed Successfully... \\n\")\n",
        "#   # print(dataset.head().to_string(header=True))\n",
        "# else:\n",
        "#   print(\"❌ No Stemming Needed...\")"
      ],
      "metadata": {
        "id": "iMWNF_e5j0vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kluQwU4vAry9"
      },
      "source": [
        "### Step-3.5: Checking any Categorical Data and Encoding them:-"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "# X = dataset.iloc[:, 1:-1]\n",
        "# X = dataset.iloc[:, :-1]\n",
        "# y = dataset.iloc[:, -1].values\n",
        "\n",
        "repeating_cols = []\n",
        "\n",
        "if categorical_cols.empty:\n",
        "  print(f\"\\t❌ No Encoding needed! No Categorical values found in whole Dataset.\\n\")\n",
        "else:\n",
        "  for col in categorical_cols:\n",
        "      print(f\"> String Values present in Column '{col}'.\")\n",
        "      # Check for repeating values within the categorical column\n",
        "      value_counts = dataset[col].value_counts()\n",
        "      repeating_values = value_counts[value_counts > 1].index.tolist()\n",
        "      if repeating_values:\n",
        "        repeating_cols.append(col)\n",
        "        # print(f\"\\t- Also, Categorical values found in column '{col}': {repeating_values}.\")\n",
        "        print(f\"\\t- Also, Categorical values found in column '{col}'.\")\n",
        "        print(f\"\\t✅ '{col}' is Encoded Successfully...\\n\")\n",
        "      else:\n",
        "        print(f\"\\t❌ But No Categorical values found in column '{col}'.\\n\")\n",
        "\n",
        "  print(\"=> Repeating Columns in Matrix of Features(X): \", repeating_cols, \"\\n\")\n",
        "  if repeating_cols:\n",
        "\n",
        "    \"\"\" OneHotEncoding: \"\"\"\n",
        "    # onehot_encoder = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), repeating_cols)], remainder='passthrough')\n",
        "    # encoder_transform = onehot_encoder.fit_transform(dataset)\n",
        "    # # Conditional conversion to dense array\n",
        "    # if scipy.sparse.issparse(encoder_transform):  # Check if sparse\n",
        "    #   dataset = encoder_transform.toarray()\n",
        "    # else:\n",
        "    #   dataset = encoder_transform\n",
        "\n",
        "\n",
        "    \"\"\" LabelEncoding: \"\"\"\n",
        "    label_encoder = LabelEncoder()\n",
        "    for col in repeating_cols:\n",
        "      dataset[col] = label_encoder.fit_transform(dataset[col])\n",
        "\n",
        "  else:\n",
        "    print(\"❌ No Repeating Columns found in Matrix of Features(X). \\n\")\n",
        "\n",
        "  # print(\"Matrix of Features(X): \\n\", pd.DataFrame(X).head().to_string(header=True))\n",
        "  # print(\"\\n\")\n",
        "  # print(\"Dependent Variable(y): \\n\", pd.DataFrame(y).head().to_string(header=False, index=False))\n"
      ],
      "metadata": {
        "id": "NF_l7fef3Dnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Step-4: Split the Dataset into the Training set and Test set:-"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# print(\"✅ Data Splitted Successfully...\\n\")\n",
        "# # print(\"Printing Training Sets: \")\n",
        "# # print(\"> X_train: \\n\", (pd.DataFrame(X_train).head()).to_string(), \"\\n\")\n",
        "# # print(\"> X_test: \\n\", (pd.DataFrame(X_test).head()).to_string(), \"\\n\")\n",
        "# # print(\"\\n\")\n",
        "# # print(\"Printing Test Sets: \")\n",
        "# # print(\"> y_train: \\n\", (pd.DataFrame(y_train).head()).to_string(header=False), \"\\n\")\n",
        "# # print(\"> y_test: \\n\", (pd.DataFrame(y_test).head()).to_string(header=False), \"\\n\")"
      ],
      "metadata": {
        "id": "4ML-zBGmBkeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Step-5: Feature Scaling:-"
      ]
    },
    {
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Create a copy for scaling:\n",
        "X_scaled = dataset.copy()\n",
        "\n",
        "# 2. Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 3. Select columns for scaling (exclude columns with only 0 and 1)\n",
        "cols_to_scale = []\n",
        "for col in X_scaled.columns:\n",
        "    if not np.all(np.isin(X_scaled[col], [0, 1])):\n",
        "        cols_to_scale.append(col)\n",
        "\n",
        "# 4. Apply scaling to selected columns in X_scaled\n",
        "X_scaled[cols_to_scale] = scaler.fit_transform(X_scaled[cols_to_scale])\n",
        "\n",
        "print(\"✅ Feature Scaling is Done Successfully...\\n\")\n",
        "print(X_scaled.head().to_string())\n",
        "\n",
        "# Now, X_scaled holds the scaled features, similar to how 'X' was used before."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-YfXPUQuXl2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Step-B: Model Builing & Evaluation...***"
      ],
      "metadata": {
        "id": "4ibPqcgELO4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-1: Find Optimal Clusters using Elbow Method:-"
      ],
      "metadata": {
        "id": "PpxqQN33mQ44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot Elbow Graph\n",
        "# plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, 11), wcss, marker='o', linestyle='--', color='b')\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r876L3fxmZqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "models = [\n",
        "    KMeans(n_clusters=5, random_state=42),\n",
        "    DBSCAN(eps=0.5, min_samples=5),\n",
        "    AgglomerativeClustering(n_clusters=5),\n",
        "    GaussianMixture(n_components=5, random_state=42)\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "  y_pred = model.fit_predict(X_scaled)\n",
        "  # print(y_pred)\n",
        "\n",
        "  if len(set(y_pred)) > 1:  # Ensure more than one cluster\n",
        "      silhouette = silhouette_score(X_scaled, y_pred)\n",
        "      db_index = davies_bouldin_score(X_scaled, y_pred)\n",
        "      calinski = calinski_harabasz_score(X_scaled, y_pred)\n",
        "\n",
        "      print(f\"✅ {model.__class__.__name__} is trained Sucessfully...\")\n",
        "      print(f\"-> {model.__class__.__name__}:-\")\n",
        "      print(f\"\\t Silhouette Score:\\t\\t  {silhouette:.4f}\")\n",
        "      print(f\"\\t Davies-Bouldin Index:\\t  {db_index:.4f}\")\n",
        "      print(f\"\\t Calinski-Harabasz Index: {calinski:.4f}\\n\")\n",
        "  else:\n",
        "      print(f\"{model.__class__.__name__}: Not enough clusters for evaluation\\n\")\n",
        "\n",
        "\n",
        "# # K-Means Clustering\n",
        "# kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "# y_pred_kmeans = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# # DBSCAN Clustering\n",
        "# dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "# y_pred_dbscan = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# # Agglomerative Clustering\n",
        "# agglo = AgglomerativeClustering(n_clusters=optimal_k)\n",
        "# y_pred_agglo = agglo.fit_predict(X_scaled)\n",
        "\n",
        "# # Gaussian Mixture Model (GMM)\n",
        "# gmm = GaussianMixture(n_components=optimal_k, random_state=42)\n",
        "# y_pred_gmm = gmm.fit_predict(X_scaled)\n"
      ],
      "metadata": {
        "id": "dQKzuZx4sCyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-2: Evaluate Clustering Performance:-"
      ],
      "metadata": {
        "id": "OMDR19HSmtpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluate_clustering(X, y_pred, model_name):\n",
        "#     if len(set(y_pred)) > 1:  # Ensure more than one cluster\n",
        "#         silhouette = silhouette_score(X, y_pred)\n",
        "#         db_index = davies_bouldin_score(X, y_pred)\n",
        "#         calinski = calinski_harabasz_score(X, y_pred)\n",
        "\n",
        "#         print(f\"{model_name}:\")\n",
        "#         print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
        "#         print(f\"  Davies-Bouldin Index: {db_index:.4f}\")\n",
        "#         print(f\"  Calinski-Harabasz Index: {calinski:.4f}\\n\")\n",
        "#     else:\n",
        "#         print(f\"{model_name}: Not enough clusters for evaluation\\n\")\n",
        "\n",
        "# # Evaluate models\n",
        "# evaluate_clustering(X_scaled, y_pred_kmeans, \"K-Means Clustering\")\n",
        "# evaluate_clustering(X_scaled, y_pred_dbscan, \"DBSCAN Clustering\")\n",
        "# evaluate_clustering(X_scaled, y_pred_agglo, \"Agglomerative Clustering\")\n",
        "# evaluate_clustering(X_scaled, y_pred_gmm, \"Gaussian Mixture Model (GMM)\")"
      ],
      "metadata": {
        "id": "sYOxoC70op2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# silhouette_scores = []\n",
        "# davies_bouldin_scores = []\n",
        "\n",
        "# for i in range(2, 11):\n",
        "#     kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
        "#     y_kmeans = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "#     silhouette = silhouette_score(X_scaled, y_kmeans)\n",
        "#     db_score = davies_bouldin_score(X_scaled, y_kmeans)\n",
        "\n",
        "#     silhouette_scores.append((i, silhouette))\n",
        "#     davies_bouldin_scores.append((i, db_score))\n",
        "\n",
        "# # Print Scores\n",
        "# print(\"> Silhouette Scores:\")\n",
        "# for i, score in silhouette_scores:\n",
        "#     print(f\"Clusters: {i}, Score: {score:.4f}\")\n",
        "\n",
        "# print(\"\\n> Davies-Bouldin Index:\")\n",
        "# for i, score in davies_bouldin_scores:\n",
        "#     print(f\"Clusters: {i}, Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "qVFr0i0GmyUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-3: Choose Best K & Train Final Model:-"
      ],
      "metadata": {
        "id": "g0Pg7YF8nKXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# final_kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42) # 4 is choosen based on Elbow, Silhouette & DB index\n",
        "# y_kmeans = final_kmeans.fit_predict(X_scaled)\n",
        "# print(y_kmeans)\n",
        "\n",
        "# print(\"\\n✅ Model Training Completed!\\n\")"
      ],
      "metadata": {
        "id": "srNH7MKCLO4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-4: Visulising the Clusters:-"
      ],
      "metadata": {
        "id": "B5ziVdxOq5Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def plot_clusters(X, y_pred, model_name):\n",
        "#     plt.figure(figsize=(8, 6))\n",
        "#     plt.scatter(X_scaled.iloc[:, 2], X_scaled.iloc[:, 3], c=y_pred, cmap='viridis', edgecolors='k')\n",
        "#     plt.xlabel(\"Annual Income (k$)\")\n",
        "#     plt.ylabel(\"Spending Score (1-100)\")\n",
        "#     plt.title(f\"{model_name} Clustering\")\n",
        "#     plt.colorbar()\n",
        "#     plt.show()\n",
        "\n",
        "# # Convert DataFrame to NumPy array for plotting\n",
        "# X_np = X_scaled.copy()\n",
        "\n",
        "# plot_clusters(X_np, y_pred_kmeans, \"K-Means\")\n",
        "# plot_clusters(X_np, y_pred_dbscan, \"DBSCAN\")\n",
        "# plot_clusters(X_np, y_pred_agglo, \"Agglomerative\")\n",
        "# plot_clusters(X_np, y_pred_gmm, \"Gaussian Mixture Model\")\n"
      ],
      "metadata": {
        "id": "FaPUCwGdou_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(10, 6))\n",
        "# linkage_matrix = linkage(X_scaled, method='ward')\n",
        "# dendrogram(linkage_matrix)\n",
        "# plt.title(\"Dendrogram for Hierarchical Clustering\")\n",
        "# plt.xlabel(\"Customers\")\n",
        "# plt.ylabel(\"Distance\")\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "r8f3S8Uhoxso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# colors = ['red', 'yellow', 'green', 'cyan', 'magenta']\n",
        "# for i in range(0, 5):\n",
        "#   plt.scatter(X_scaled[y_kmeans == i, 0], X_scaled[y_kmeans == i, 1], s = 100, c = colors[i], label = f'Cluster {i+1}')\n",
        "\n",
        "# plt.scatter(final_kmeans.cluster_centers_[:, 0], final_kmeans.cluster_centers_[:, 1], s=300, c='blue', label='Centroids')\n",
        "\n",
        "# plt.title('Clusters of customers')\n",
        "# plt.xlabel('Annual Income (k$)')\n",
        "# plt.ylabel('Spending Score (1-100)')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "sQuSZK13rDgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# # Apply PCA to reduce to 2 dimensions for visualization\n",
        "# pca = PCA(n_components=4)\n",
        "# X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# colors = ['red', 'yellow', 'green', 'cyan', 'magenta']\n",
        "# for i in range(0, 5):\n",
        "#   # Get the data points for the current cluster\n",
        "#   cluster_data = X_pca[y_kmeans == i]\n",
        "#   # Extract the values for the first and second columns (Annual Income and Spending Score)\n",
        "#   plt.scatter(cluster_data[:, 1], cluster_data[:, 2], s=100, c=colors[i], label=f'Cluster {i+1}')\n",
        "\n",
        "# plt.scatter(final_kmeans.cluster_centers_[:, 0], final_kmeans.cluster_centers_[:, 1], s=300, c='blue', label='Centroids')\n",
        "\n",
        "# plt.title('Clusters of customers')\n",
        "# plt.xlabel('Annual Income (k$)')\n",
        "# plt.ylabel('Spending Score (1-100)')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mOU2w1U8bbiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Step-C: Saving the Model & Testing It...***"
      ],
      "metadata": {
        "id": "OyKDXbZPLO4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-1: Saving the Model(.sav):-"
      ],
      "metadata": {
        "id": "5wXV-ntdLO4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "\n",
        "# # Train and save models\n",
        "# for model in models:\n",
        "#     # model.fit(X_train, y_train)\n",
        "\n",
        "#     # Save each model with its name\n",
        "#     model_filename = f\"{model.__class__.__name__}.sav\"\n",
        "#     pickle.dump(model, open(model_filename, \"wb\"))\n",
        "\n",
        "#     print(f\"✅ Model saved: {model_filename}\")"
      ],
      "metadata": {
        "id": "3p8QgiqoLO4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-2: Predicting and Testing the Model:-"
      ],
      "metadata": {
        "id": "LTFD2MJYLO4y"
      }
    },
    {
      "source": [
        "# correct_predictions = 0\n",
        "# max_correct_predictions = 0\n",
        "# model_predictions = {} # dictionary to store model and their correct predictions\n",
        "\n",
        "# random_input = np.random.randint(2, 5)\n",
        "# # random_input = np.random.randint(2, 10)\n",
        "# print(\"Number of Inputs for Predction: \", random_input, \"\\n\")\n",
        "\n",
        "# for i in range(random_input):\n",
        "\n",
        "#   sample_input = pd.DataFrame(X_train).iloc[i].values.reshape(1, -1)  # Convert a row to an array\n",
        "#   sample_output = round(y_train[i], 2)\n",
        "\n",
        "#   print(\"Before Predicting, Sample Data & Output Sales are:-\")\n",
        "#   print(\"> Sample Data: \", sample_input)\n",
        "#   print(\"> Output: \",sample_output, \"\\n\")\n",
        "\n",
        "#   for model in models:\n",
        "#     model = pickle.load(open(f\"{model.__class__.__name__}.sav\", \"rb\"))\n",
        "#     model_filename = f\"{model.__class__.__name__}.sav\"\n",
        "#     print(f\"-> {model.__class__.__name__}:-\")\n",
        "\n",
        "#     prediction = model.predict(sample_input)\n",
        "#     # print(\" - Predicted Sales:\", prediction[0])\n",
        "#     rounded_prediction = round(prediction[0], 2)\n",
        "#     print(f\" - Predicted Sales: {rounded_prediction:.2f}\")\n",
        "\n",
        "#     if rounded_prediction == sample_output:\n",
        "#         correct_predictions += 1\n",
        "#         print(\"\\t✅ Nice Work! your Prediction is correct...\\n\")\n",
        "#         # increment correct prediction count for the model in the dictionary\n",
        "#         model_predictions[model.__class__.__name__] = model_predictions.get(model.__class__.__name__, 0) + 1\n",
        "#     else:\n",
        "#         print(\"\\t❌ Oops! your Prediction is incorrect...\\n\")\n",
        "\n",
        "#   print(\"=\"*75, \"\\n\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "43VeoJRYLO4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-3: Selecting the Perfect Model for this Dataset:-"
      ],
      "metadata": {
        "id": "VMa62w9bLbkO"
      }
    },
    {
      "source": [
        "# # Instead of using max, we'll iterate and check for perfect predictions\n",
        "# best_models = []  # List to store all perfectly predicting models\n",
        "\n",
        "# for model_name, correct_count in model_predictions.items():\n",
        "#     if correct_count == random_input:  # Assuming you're testing on 5 samples\n",
        "#         best_models.append(model_name)\n",
        "\n",
        "# # Print the results\n",
        "# if best_models:\n",
        "#     print(\"The following Models achieved all perfect Predictions:\")\n",
        "#     for model_name in best_models:\n",
        "#       print(f\"\\t- ✅ '{model_name}:\\n\\t\\t\\t\\t with '{model_predictions[model_name]}' correct predictions\")\n",
        "# else:\n",
        "#     print(\"No models achieved perfect predictions on all samples.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WC23VxLpz1U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# # Instead of using max, we'll iterate and check for at least one correct prediction\n",
        "# models_with_correct_predictions = []  # List to store models with at least one correct prediction\n",
        "\n",
        "# for model_name, correct_count in model_predictions.items():\n",
        "#     if correct_count >= 1:  # Check for at least one correct prediction\n",
        "#         models_with_correct_predictions.append(model_name)\n",
        "\n",
        "# # Print the results\n",
        "# if models_with_correct_predictions:\n",
        "#     print(\"The following Models achieved at least one correct Prediction:\")\n",
        "#     for model_name in models_with_correct_predictions:\n",
        "#         print(f\"\\t- ✅ '{model_name}':\\n\\t\\t\\t '{model_predictions[model_name]}' out of {random_input} correct predictions\")\n",
        "# else:\n",
        "#     print(\"No models achieved any correct predictions on the samples.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "TWHQXwR8nIhh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}